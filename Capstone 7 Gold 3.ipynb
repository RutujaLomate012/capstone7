{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6fbd39d-0895-4c22-b0e3-6c75ccd311ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import datetime as dt\n",
    "\n",
    "# Source tables\n",
    "docs = spark.table(\"knowledgehub_lakehouse.silver.docs_clean\")\n",
    "events = spark.table(\"knowledgehub_lakehouse.silver.access_events_clean\")\n",
    "\n",
    "# Collect all dates dynamically\n",
    "all_dates = (\n",
    "    docs.select(F.to_date(\"created_ts\").alias(\"d\"))\n",
    "    .union(docs.select(F.to_date(\"updated_ts\").alias(\"d\")))\n",
    "    .union(events.select(F.to_date(\"event_ts\").alias(\"d\")))\n",
    "    .where(\"d is not null\")\n",
    ")\n",
    "\n",
    "min_date = all_dates.agg(F.min(\"d\")).first()[0]\n",
    "max_date = all_dates.agg(F.max(\"d\")).first()[0]\n",
    "\n",
    "# Buffer for analytics\n",
    "start_date = (min_date - dt.timedelta(days=30)).isoformat()\n",
    "end_date   = (max_date + dt.timedelta(days=365)).isoformat()\n",
    "\n",
    "# -------------------------------\n",
    "# DIM DATE\n",
    "# -------------------------------\n",
    "dim_date = (\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT explode(\n",
    "            sequence(\n",
    "                to_date('{start_date}'),\n",
    "                to_date('{end_date}'),\n",
    "                interval 1 day\n",
    "            )\n",
    "        ) AS date\n",
    "    \"\"\")\n",
    "    # Surrogate Key\n",
    "    .withColumn(\"date_key\", F.date_format(\"date\", \"yyyyMMdd\").cast(\"int\"))\n",
    "\n",
    "    # Day attributes\n",
    "    .withColumn(\"day\", F.dayofmonth(\"date\"))\n",
    "    .withColumn(\"day_name\", F.date_format(\"date\", \"EEEE\"))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"date\"))\n",
    "\n",
    "    # Week attributes\n",
    "    .withColumn(\"week_of_year\", F.weekofyear(\"date\"))\n",
    "\n",
    "    # Month attributes\n",
    "    .withColumn(\"month\", F.month(\"date\"))\n",
    "    .withColumn(\"month_name\", F.date_format(\"date\", \"MMMM\"))\n",
    "\n",
    "    # Quarter attributes\n",
    "    .withColumn(\"quarter\", F.quarter(\"date\"))\n",
    "\n",
    "    # Year attributes\n",
    "    .withColumn(\"year\", F.year(\"date\"))\n",
    "\n",
    "    # Flags\n",
    "    .withColumn(\n",
    "        \"is_weekend\",\n",
    "        F.when(F.dayofweek(\"date\").isin([1, 7]), 1).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write to Gold\n",
    "dim_date.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\n",
    "    \"knowledgehub_lakehouse.gold.dim_date\"\n",
    ")\n",
    "\n",
    "\n",
    "display(spark.table(\"knowledgehub_lakehouse.gold.dim_date\").limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c87ad0-7fca-499a-b647-00018c322256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "dept_src = spark.table(\"knowledgehub_lakehouse.reference.departments\")\n",
    "\n",
    "dept_clean = (\n",
    "    dept_src\n",
    "    .select(\n",
    "        F.trim(F.col(\"department\")).alias(\"department_name\"),\n",
    "        F.col(\"owner_group\").cast(\"string\"),\n",
    "        F.col(\"sensitivity_level\").cast(\"string\")\n",
    "    )\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(\"department_name\")\n",
    "\n",
    "dim_department = (\n",
    "    dept_clean\n",
    "    .withColumn(\"department_key\", F.row_number().over(window_spec))\n",
    ")\n",
    "\n",
    "dim_department.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"knowledgehub_lakehouse.gold.dim_department\")\n",
    "\n",
    "display(spark.table(\"knowledgehub_lakehouse.gold.dim_department\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e041f2-077f-43a2-a2e7-c4a5c20a340e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "doc_type_src = spark.table(\"knowledgehub_lakehouse.reference.doc_type_rules\")\n",
    "\n",
    "doc_type_clean = (\n",
    "    doc_type_src\n",
    "    .select(\n",
    "        F.upper(F.trim(F.col(\"doc_type\"))).alias(\"doc_type\"),\n",
    "        F.col(\"mandatory_fields\").cast(\"string\"),\n",
    "        F.col(\"retention_days\").cast(\"int\"),\n",
    "        F.col(\"allowed_confidentiality\").cast(\"string\")\n",
    "    )\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(\"doc_type\")\n",
    "\n",
    "dim_doc_type = (\n",
    "    doc_type_clean\n",
    "    .withColumn(\"doc_type_key\", F.row_number().over(window_spec))\n",
    ")\n",
    "\n",
    "dim_doc_type.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"knowledgehub_lakehouse.gold.dim_doc_type\")\n",
    "\n",
    "display(spark.table(\"knowledgehub_lakehouse.gold.dim_doc_type\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed9cdfa8-2fa7-4e89-8636-dbfab307647b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770293550024}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Source tables\n",
    "docs = spark.table(\"knowledgehub_lakehouse.silver.docs_clean\")\n",
    "dim_department = spark.table(\"knowledgehub_lakehouse.gold.dim_department\")\n",
    "dim_doc_type = spark.table(\"knowledgehub_lakehouse.gold.dim_doc_type\")\n",
    "\n",
    "fact_doc_versions = (\n",
    "    docs\n",
    "    # -------------------------\n",
    "    # Join dimensions\n",
    "    # -------------------------\n",
    "    .join(\n",
    "        dim_department,\n",
    "        docs.department == dim_department.department_name,\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        dim_doc_type,\n",
    "        docs.doc_type == dim_doc_type.doc_type,\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Date surrogate key\n",
    "    # -------------------------\n",
    "    .withColumn(\n",
    "        \"date_key\",\n",
    "        F.date_format(F.to_date(\"updated_ts\"), \"yyyyMMdd\").cast(\"int\")\n",
    "    )\n",
    "    .withColumn( \"freshness_days\", \n",
    "                F.datediff(F.current_date(), F.to_date(\"updated_ts\")) ) \n",
    "    .withColumn( \"missing_metadata_count\", \n",
    "                ( F.when(F.col(\"department\").isNull(), 1).otherwise(0) + \n",
    "                 F.when(F.col(\"dim_doc_type.doc_type\").isNull(), 1).otherwise(0) + \n",
    "                 F.when(F.col(\"confidentiality\").isNull(), 1).otherwise(0) ) ) \n",
    "    .withColumn( \"text_length_bucket\", \n",
    "                F.when(F.length(\"doc_text\") < 500, \"SMALL\") .\n",
    "                when(F.length(\"doc_text\") < 2000, \"MEDIUM\") .otherwise(\"LARGE\") )\n",
    "\n",
    "    # -------------------------\n",
    "    # Final fact projection\n",
    "    # -------------------------\n",
    "    .select(\n",
    "        # Natural document keys\n",
    "        \"doc_id\",\n",
    "        \"version_norm\",\n",
    "        \"current_version_flag\",\n",
    "\n",
    "        # Surrogate dimension keys\n",
    "        \"department_key\",\n",
    "        \"doc_type_key\",\n",
    "        \"date_key\",\n",
    "\n",
    "        # metadata\n",
    "        \"freshness_days\",\n",
    "        \"missing_metadata_count\",\n",
    "        \"text_length_bucket\",\n",
    "    \n",
    "        # Business attributes\n",
    "        \"doc_title\",\n",
    "        \"owner_user\",\n",
    "        \"status\",\n",
    "        \"confidentiality\",\n",
    "        \"policy_region\",\n",
    "\n",
    "        # Audit / lineage\n",
    "        \"source_system\",\n",
    "        \"source_file\",\n",
    "        \"input_batch\",\n",
    "        \"ingest_ts\",\n",
    "\n",
    "        # Timestamps\n",
    "        \"created_ts\",\n",
    "        \"updated_ts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(fact_doc_versions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8afca4-7bda-4d1d-918e-5d98f6f3e9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_doc_versions.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"knowledgehub_lakehouse.gold.fact_doc_versions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91dc836-63c1-4543-a3f2-af5cc328880a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# docs = spark.table(\"knowledgehub_lakehouse.silver.docs_clean\")\n",
    "# dim_department = spark.table(\"knowledgehub_lakehouse.gold.dim_department\")\n",
    "# dim_doc_type = spark.table(\"knowledgehub_lakehouse.gold.dim_doc_type\")\n",
    "\n",
    "# window_spec = Window.partitionBy(\"doc_id\", \"version_norm\") \\\n",
    "#                     .orderBy(F.col(\"updated_ts\").desc(), F.col(\"ingest_ts\").desc())\n",
    "\n",
    "# src = (\n",
    "#     docs\n",
    "#     .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "#     .filter(\"rn = 1\")\n",
    "#     .join(dim_department, docs.department == dim_department.department_name, \"left\")\n",
    "#     .join(dim_doc_type, docs.doc_type == dim_doc_type.doc_type, \"left\")\n",
    "#     .withColumn(\"freshness_days\", F.datediff(F.current_date(), F.to_date(\"updated_ts\")))\n",
    "#     .withColumn(\n",
    "#         \"missing_metadata_count\",\n",
    "#         (docs.department.isNull().cast(\"int\") +\n",
    "#          docs.doc_type.isNull().cast(\"int\") +\n",
    "#          docs.confidentiality.isNull().cast(\"int\"))\n",
    "#     )\n",
    "#     .withColumn(\n",
    "#         \"text_length_bucket\",\n",
    "#         F.when(F.length(\"doc_text\") < 500, \"SMALL\")\n",
    "#          .when(F.length(\"doc_text\") < 2000, \"MEDIUM\")\n",
    "#          .otherwise(\"LARGE\")\n",
    "#     )\n",
    "#     .withColumn(\n",
    "#         \"date_key\",\n",
    "#         F.date_format(F.to_date(\"updated_ts\"), \"yyyyMMdd\").cast(\"int\")\n",
    "#     )\n",
    "#     .select(\n",
    "#         \"doc_id\",\n",
    "#         \"version_norm\",\n",
    "#         \"current_version_flag\",\n",
    "#         \"department_key\",\n",
    "#         \"doc_type_key\",\n",
    "#         \"date_key\",\n",
    "#         \"doc_title\",\n",
    "#         \"owner_user\",\n",
    "#         \"status\",\n",
    "#         \"confidentiality\",\n",
    "#         \"policy_region\",\n",
    "#         \"source_system\",\n",
    "#         \"source_file\",\n",
    "#         \"input_batch\",\n",
    "#         \"ingest_ts\",\n",
    "#         \"created_ts\",\n",
    "#         \"updated_ts\",\n",
    "#         \"freshness_days\", \n",
    "#         \"missing_metadata_count\",\n",
    "#         \"text_length_bucket\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# src.createOrReplaceTempView(\"src_fact_docs\")\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# MERGE INTO knowledgehub_lakehouse.gold.fact_doc_versions tgt\n",
    "# USING src_fact_docs src\n",
    "# ON tgt.doc_id = src.doc_id\n",
    "# AND tgt.version_norm = src.version_norm\n",
    "\n",
    "# WHEN MATCHED AND src.updated_ts > tgt.updated_ts THEN\n",
    "#   UPDATE SET *\n",
    "\n",
    "# WHEN NOT MATCHED THEN\n",
    "#   INSERT *\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d3e5ec-e29b-4b1f-ad92-ad395d3f49d6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770118073488}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT doc_id, COUNT(*) \n",
    "FROM knowledgehub_lakehouse.gold.fact_doc_versions\n",
    "GROUP BY doc_id\n",
    "ORDER BY COUNT(*) DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6de0b35-3ed6-4dbe-a206-052f600923e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 11"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Read source users (latest snapshot)\n",
    "# -------------------------------------------------\n",
    "src_users = (\n",
    "    spark.table(\"knowledgehub_lakehouse.reference.users\")\n",
    "    .select(\n",
    "        F.col(\"user_id\").cast(\"string\"),\n",
    "        F.col(\"role\").cast(\"string\"),\n",
    "        F.col(\"department\").cast(\"string\"),\n",
    "        F.col(\"region\").cast(\"string\")\n",
    "    )\n",
    "    .dropDuplicates([\"user_id\"])\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Read existing dimension (if exists)\n",
    "# -------------------------------------------------\n",
    "if spark.catalog.tableExists(\"knowledgehub_lakehouse.gold.dim_user\"):\n",
    "    tgt_users = spark.table(\"knowledgehub_lakehouse.gold.dim_user\")\n",
    "else:\n",
    "    tgt_users = None\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. If table does not exist â†’ Initial load\n",
    "# -------------------------------------------------\n",
    "if tgt_users is None:\n",
    "    dim_user = (\n",
    "        src_users\n",
    "        .withColumn(\"user_key\", F.monotonically_increasing_id())\n",
    "        .withColumn(\"effective_from\", F.current_date())\n",
    "        .withColumn(\"effective_to\", F.lit(None).cast(\"date\"))\n",
    "        .withColumn(\"is_current\", F.lit(True))\n",
    "    )\n",
    "\n",
    "    dim_user.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\n",
    "        \"knowledgehub_lakehouse.gold.dim_user\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # -------------------------------------------------\n",
    "    # 4. Identify changed records (SCD Type 2)\n",
    "    # -------------------------------------------------\n",
    "    current_dim = tgt_users.filter(F.col(\"is_current\") == True)\n",
    "\n",
    "    changes = (\n",
    "        src_users.alias(\"src\")\n",
    "        .join(\n",
    "            current_dim.alias(\"tgt\"),\n",
    "            on=\"user_id\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .filter(\n",
    "            (F.col(\"src.role\") != F.col(\"tgt.role\")) |\n",
    "            (F.col(\"src.department\") != F.col(\"tgt.department\")) |\n",
    "            (F.col(\"src.region\") != F.col(\"tgt.region\"))\n",
    "        )\n",
    "        .select(F.col(\"tgt.user_key\"), F.lit(True).alias(\"changed_flag\"))\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 5. Expire old records\n",
    "    # -------------------------------------------------\n",
    "    expired = (\n",
    "        tgt_users\n",
    "        .join(changes, on=\"user_key\", how=\"left\")\n",
    "        .withColumn(\n",
    "            \"effective_to\",\n",
    "            F.when(F.col(\"changed_flag\") == True, F.current_date() - F.expr(\"INTERVAL 1 DAY\"))\n",
    "             .otherwise(F.col(\"effective_to\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"is_current\",\n",
    "            F.when(F.col(\"changed_flag\") == True, F.lit(False))\n",
    "             .otherwise(F.col(\"is_current\"))\n",
    "        )\n",
    "        .drop(\"changed_flag\")\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 6. Insert new versions for changed & new users\n",
    "    # -------------------------------------------------\n",
    "    new_versions = (\n",
    "        src_users.alias(\"src\")\n",
    "        .join(\n",
    "            current_dim.alias(\"tgt\"),\n",
    "            on=\"user_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .filter(\n",
    "            F.col(\"tgt.user_id\").isNull() |  # brand new user\n",
    "            (F.col(\"src.role\") != F.col(\"tgt.role\")) |\n",
    "            (F.col(\"src.department\") != F.col(\"tgt.department\")) |\n",
    "            (F.col(\"src.region\") != F.col(\"tgt.region\"))\n",
    "        )\n",
    "        .select(\n",
    "            F.monotonically_increasing_id().alias(\"user_key\"),\n",
    "            F.col(\"src.user_id\"),\n",
    "            F.col(\"src.role\"),\n",
    "            F.col(\"src.department\"),\n",
    "            F.col(\"src.region\"),\n",
    "            F.current_date().alias(\"effective_from\"),\n",
    "            F.lit(None).cast(\"date\").alias(\"effective_to\"),\n",
    "            F.lit(True).alias(\"is_current\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 7. Union expired + new versions and overwrite\n",
    "    # -------------------------------------------------\n",
    "    final_dim_user = expired.unionByName(new_versions)\n",
    "    \n",
    "    # Break lineage to avoid reading from table being overwritten\n",
    "    final_dim_user.createOrReplaceTempView(\"temp_dim_user\")\n",
    "    \n",
    "    spark.table(\"temp_dim_user\").write.mode(\"overwrite\").format(\"delta\").saveAsTable(\n",
    "        \"knowledgehub_lakehouse.gold.dim_user\"\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8. Validate\n",
    "# -------------------------------------------------\n",
    "display(\n",
    "    spark.table(\"knowledgehub_lakehouse.gold.dim_user\")\n",
    "    .orderBy(\"user_id\", \"effective_from\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29049fd-c620-4bdb-a85b-8cb250e02dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Read tables\n",
    "# --------------------------------------------------\n",
    "events = spark.table(\"knowledgehub_lakehouse.silver.access_events_clean\")\n",
    "\n",
    "dim_user = (\n",
    "    spark.table(\"knowledgehub_lakehouse.gold.dim_user\")\n",
    "    .filter(\"is_current = true\")\n",
    "    .select(\"user_id\", \"user_key\")\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Filter non-orphan & deduplicate\n",
    "# --------------------------------------------------\n",
    "events_dedup = (\n",
    "    events\n",
    "    .filter(\"orphan_doc_flag = 0\")\n",
    "    .dropDuplicates([\"event_id\"])\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Join with dim_user & add date_key\n",
    "# --------------------------------------------------\n",
    "fact_access_events = (\n",
    "    events_dedup\n",
    "    .join(dim_user, on=\"user_id\", how=\"left\")\n",
    "    .withColumn(\"date_key\", F.date_format(F.to_date(\"event_ts\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    .select(\n",
    "        \"doc_id\",\n",
    "        \"action\",\n",
    "        \"client_type\",\n",
    "        \"event_id\",\n",
    "        \"event_ts\",\n",
    "        \"result_count\",\n",
    "        \"search_query\",\n",
    "        \"user_key\",                # surrogate key\n",
    "        \"_rescued_data\",\n",
    "        \"ingest_ts\",\n",
    "        \"source_file\",\n",
    "        \"input_batch\",\n",
    "        \"orphan_doc_flag\",\n",
    "        \"search_success_flag\",\n",
    "        \"date_key\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Write to GOLD fact table\n",
    "# --------------------------------------------------\n",
    "fact_access_events.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\n",
    "    \"knowledgehub_lakehouse.gold.fact_access_events\"\n",
    ")\n",
    "\n",
    "display(fact_access_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3945ef-53a0-4b23-b7a0-297ce3418666",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix MERGE for user_key and date_key"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# MERGE INTO knowledgehub_lakehouse.gold.fact_access_events tgt\n",
    "# USING (\n",
    "#   SELECT \n",
    "#     src.doc_id,\n",
    "#     src.action,\n",
    "#     src.client_type,\n",
    "#     src.event_id,\n",
    "#     src.event_ts,\n",
    "#     src.result_count,\n",
    "#     src.search_query,\n",
    "#     u.user_key,\n",
    "#     src._rescued_data,\n",
    "#     src.ingest_ts,\n",
    "#     src.source_file,\n",
    "#     src.input_batch,\n",
    "#     src.orphan_doc_flag,\n",
    "#     src.search_success_flag,\n",
    "#     CAST(date_format(to_date(src.event_ts), 'yyyyMMdd') AS INT) AS date_key\n",
    "#   FROM knowledgehub_lakehouse.silver.access_events_clean src\n",
    "#   LEFT JOIN knowledgehub_lakehouse.gold.dim_user u\n",
    "#     ON src.user_id = u.user_id AND u.is_current = true\n",
    "# ) src\n",
    "# ON tgt.event_id = src.event_id\n",
    "# WHEN NOT MATCHED AND src.orphan_doc_flag = 0 THEN\n",
    "#   INSERT (\n",
    "#     doc_id, action, client_type, event_id, event_ts, result_count, search_query, user_key, _rescued_data, ingest_ts, source_file, input_batch, orphan_doc_flag, search_success_flag, date_key\n",
    "#   )\n",
    "#   VALUES (\n",
    "#     src.doc_id, src.action, src.client_type, src.event_id, src.event_ts, src.result_count, src.search_query, src.user_key, src._rescued_data, src.ingest_ts, src.source_file, src.input_batch, src.orphan_doc_flag, src.search_success_flag, src.date_key\n",
    "#   );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8512bb8-9af4-44cf-80c8-64bc1d16ac12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "usage_daily = (\n",
    "    spark.table(\"knowledgehub_lakehouse.gold.fact_access_events\")\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_ts\"))\n",
    "    .groupBy(\"doc_id\", \"event_date\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_events\"),\n",
    "        F.countDistinct(\"user_key\").alias(\"unique_users\"),\n",
    "        F.sum(F.when(F.col(\"action\") == \"SEARCH\", 1).otherwise(0)).alias(\"searches\"),\n",
    "        F.sum(F.when(F.col(\"action\") == \"VIEW\", 1).otherwise(0)).alias(\"views\"),\n",
    "        F.sum(F.when(F.col(\"action\") == \"DOWNLOAD\", 1).otherwise(0)).alias(\"downloads\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35dc1ab9-019e-47d2-9d6f-f412c80e0781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_daily.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .saveAsTable(\"knowledgehub_lakehouse.gold.fact_doc_daily_usage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c9e7668-784e-4214-92a4-0c3be95e6565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE knowledgehub_lakehouse.gold.fact_doc_versions\n",
    "ZORDER BY (doc_id);\n",
    "\n",
    "OPTIMIZE knowledgehub_lakehouse.gold.fact_access_events\n",
    "ZORDER BY (doc_id, event_ts);\n",
    "\n",
    "OPTIMIZE knowledgehub_lakehouse.gold.fact_doc_daily_usage\n",
    "ZORDER BY (doc_id);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a0d5b7-0b78-4bc9-8b7a-709a3a20f724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT doc_id, SUM(total_events) AS total_events\n",
    "FROM knowledgehub_lakehouse.gold.fact_doc_daily_usage\n",
    "GROUP BY doc_id\n",
    "ORDER BY total_events DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6143ebc-9ef3-4468-84d2-bb7f1dd73fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY knowledgehub_lakehouse.gold.fact_doc_versions;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "205b4563-613c-4a38-8d91-09abcf168316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  doc_id,\n",
    "  version_norm,\n",
    "  current_version_flag,\n",
    "  updated_ts\n",
    "FROM knowledgehub_lakehouse.gold.fact_doc_versions\n",
    "VERSION AS OF 1\n",
    "WHERE doc_id = 'DOC00875';\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4717319979303882,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone 7 Gold 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
